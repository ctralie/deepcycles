{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61284604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from topologylayer.nn import AlphaLayer, BarcodePolyFeature\n",
    "from gtda.plotting import plot_point_cloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import convolve\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transpose, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.transpose(x, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_periods = 50\n",
    "samples_per_period = 30\n",
    "N = n_periods*samples_per_period\n",
    "t = np.linspace(0, 2*np.pi*n_periods, N)\n",
    "x = np.sin(t) + 0.01*t\n",
    "x = x-np.min(x)\n",
    "x /= np.max(x)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from persim import plot_diagrams\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.BatchNorm1d(1),\n",
    "    nn.Conv1d(1, 2, samples_per_period, stride=1, bias=False),\n",
    "    nn.Flatten(start_dim=0, end_dim=1),\n",
    "    Transpose(),\n",
    "    #nn.BatchNorm1d(2),\n",
    "    AlphaLayer(maxdim=1)\n",
    ")\n",
    "data = torch.from_numpy((100*np.sin(t)).reshape(1, 1, -1)).float()\n",
    "res = model(data)\n",
    "dgm1 = model(data)[0][1].detach().numpy()\n",
    "plot_diagrams(dgm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bd7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingAutoencoder(nn.Module):\n",
    "    def __init__(self, x, dim, win, lam=1, lr=1e-2):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: ndarray(N)\n",
    "            Time series\n",
    "        dim: int\n",
    "            Dimension of embedding\n",
    "        win: int\n",
    "            Window size\n",
    "        lam: float\n",
    "            Weight of topological regularization\n",
    "        lr: float\n",
    "            Learning rate\n",
    "        \"\"\"\n",
    "        super(SlidingAutoencoder, self).__init__()\n",
    "        self.x_orig = x\n",
    "        self.x = torch.from_numpy(x.reshape(1, 1, -1)).float()\n",
    "        self.win = win\n",
    "        self.dim = dim\n",
    "        self.lam = lam\n",
    "        ## TODO: This could be an RNN, and there could also be more layers\n",
    "        self.linear1 = nn.Conv1d(1, 1, win+1, stride=1, padding='same')\n",
    "        self.linear1_relu = nn.ReLU()\n",
    "        self.norm1 = nn.BatchNorm1d(1)\n",
    "        self.conv1 = nn.Conv1d(1, dim, win, stride=1, bias=False)\n",
    "        self.flatten = nn.Flatten(start_dim=0, end_dim=1)\n",
    "        self.transpose = Transpose()\n",
    "        ## TODO: This needs to do z normalization of the point cloud\n",
    "        self.norm2 = nn.BatchNorm1d(dim) \n",
    "        self.alpha = AlphaLayer(maxdim=1)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        # Loss functions and losses\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.maxh1_loss = BarcodePolyFeature(1,2,0)\n",
    "        self.losses = []\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        y: The warped time series\n",
    "        dgms: The persistence diagrams of the sliding widow embedding of the\n",
    "              warped time series\n",
    "        \"\"\"\n",
    "        y = self.linear1(self.x)\n",
    "        y = self.linear1_relu(y)\n",
    "        #y = self.norm1(y)\n",
    "        sw = self.conv1(y)\n",
    "        sw = self.flatten(sw)\n",
    "        sw = self.transpose(sw)\n",
    "        sw = self.norm2(sw)\n",
    "        dgms = self.alpha(sw)\n",
    "        return y, dgms\n",
    "    \n",
    "    def train_step(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.train()\n",
    "        y, dgms = self.forward()\n",
    "        loss = self.mse_loss(self.x, y)-self.lam*self.maxh1_loss(dgms)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.losses.append(loss.item())\n",
    "    \n",
    "    def train_epochs(self, num_epochs):\n",
    "        self.losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train_step()\n",
    "            if epoch%50 == 0:\n",
    "                y = self.forward()[0].detach().numpy().flatten()\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                plt.plot(self.x_orig)\n",
    "                plt.plot(y)\n",
    "                plt.title(\"{}: {:.3f}\".format(epoch, self.losses[-1]))\n",
    "                plt.show()\n",
    "        y = self.forward()[0].detach().numpy().flatten()\n",
    "        return y\n",
    "\n",
    "autoencoder = SlidingAutoencoder(x, lam=1, dim=2, win=samples_per_period)\n",
    "y = autoencoder.train_epochs(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01b0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
